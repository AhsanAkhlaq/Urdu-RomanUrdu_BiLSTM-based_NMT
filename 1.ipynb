{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18250e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "# credits to this stackoverflow answer https://stackoverflow.com/questions/1936466/beautifulsoup-grab-visible-webpage-text\n",
    "\n",
    "allowed_sections=['style', 'script', 'head', 'title', 'meta', '[document]']\n",
    "\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in allowed_sections:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd67d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_with_newlines(elem):\n",
    "    text = ''\n",
    "    for e in elem.descendants:\n",
    "        if isinstance(e, str):\n",
    "            text += e\n",
    "        elif e.name == 'br' or e.name == 'p':\n",
    "            text += '\\n'\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_ghazal(url):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup= BeautifulSoup(html, 'html.parser')\n",
    "    mydivs = soup.find(\"div\", {\"class\": \"pMC\"})\n",
    "    \n",
    "    #this section removes some of the English translations present on the webpage\n",
    "    # mixing language data would add noise, and make it difficult for the model to learn\n",
    "    #BUT in future these urdu to english translations could be a valuable resource to prepare machine translation data \n",
    "    for div in mydivs.find_all(\"div\", {'class':'t'}): \n",
    "        div.decompose()\n",
    "    \n",
    "    mydivs= text_with_newlines(mydivs)\n",
    "    return mydivs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3dadd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the homepage for parsing all poet names now\n",
    "#for ghalib\n",
    "\n",
    "def parse_webpage_at_given_scroll(html):\n",
    "    ctr=0\n",
    "    soup= BeautifulSoup(html, 'html.parser')\n",
    "    mydivs = soup.find(\"div\", {\"class\": \"contentListBody\"})\n",
    "    titles=[]\n",
    "    for a in mydivs.find_all('a', href=True):\n",
    "        t=a['href']\n",
    "        if t not in titles:\n",
    "            if ctr%5==0:\n",
    "                print(\"Found the URL:\", t)\n",
    "            titles.append(t)\n",
    "            ctr+=1\n",
    "    print('=============================')    \n",
    "    print('number of titles',len(titles))\n",
    "    print('=============================')\n",
    "    \n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e91f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#language argument can be ur or hi for urdu or hindi\n",
    "import os\n",
    "def read_and_write_web(author,language='ur'):\n",
    "    \n",
    "    lang=language\n",
    "    #author='mirza-ghalib'\n",
    "    author_lan=author+'/'+lang\n",
    "    if not os.path.exists(author_lan):\n",
    "        os.makedirs(author_lan)\n",
    "\n",
    "    for url in titles:\n",
    "        name_poem=url.split('https://www.rekhta.org/ghazals/')[1]\n",
    "        path_poem= author_lan+'/'+name_poem\n",
    "        if os.path.exists(path_poem):\n",
    "            pass\n",
    "        else:\n",
    "            f= open(path_poem,\"w+\")\n",
    "            if lang=='en':\n",
    "                    url_for_lang= url\n",
    "            else:\n",
    "                url_for_lang= url+'?lang='+lang\n",
    "            ghazal = parse_ghazal(url_for_lang)\n",
    "            f.write(ghazal)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4d20cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 poets on the initial page load.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def get_all_poets():\n",
    "    # Base URL for poets index page\n",
    "    poets_url = 'https://www.rekhta.org/poets'\n",
    "    \n",
    "    # Read the HTML from the URL\n",
    "    try:\n",
    "        html = urllib.request.urlopen(poets_url).read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return []\n",
    "        \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Find all poet links\n",
    "    poets = []\n",
    "    # The links have the class 'poetName'\n",
    "    poet_links = soup.find_all('a', class_='poetName')\n",
    "    \n",
    "    for link in poet_links:\n",
    "        # Extract poet name from the 'title' attribute\n",
    "        poet_name = link.get('title')\n",
    "        poets.append(poet_name)\n",
    "    \n",
    "    print(f\"Found {len(poets)} poets on the initial page load.\")\n",
    "    return poets\n",
    "\n",
    "# Example usage\n",
    "poet_list = get_all_poets()\n",
    "print(poet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b94f7cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 poets\n"
     ]
    }
   ],
   "source": [
    "authors = get_all_poets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Parsing based on home page of authors\n",
    "url_base='https://www.rekhta.org/poets/'\n",
    "\n",
    "## TODO : Later\n",
    "## or just iterate through the list of all poets on the index, instead of hand curated list\n",
    "authors=['mirza-ghalib','allama-iqbal','faiz-ahmad-faiz','sahir-ludhianvi','meer-taqi-meer',\n",
    "         'dagh-dehlvi','kaifi-azmi','gulzar','bahadur-shah-zafar','parveen-shakir',\n",
    "         'jaan-nisar-akhtar','javed-akhtar','jigar-moradabadi','jaun-eliya',\n",
    "         'ahmad-faraz','meer-anees','mohsin-naqvi','firaq-gorakhpuri','fahmida-riaz','wali-mohammad-wali',\n",
    "        'waseem-barelvi','akbar-allahabadi','altaf-hussain-hali','ameer-khusrau','naji-shakir','naseer-turabi'\n",
    "        ,'nazm-tabatabai','nida-fazli','noon-meem-rashid','habib-jalib']\n",
    "\n",
    "\n",
    "\n",
    "for author in authors:\n",
    "    url_home_page= url_base +author+ '/ghazals'\n",
    "    html = urllib.request.urlopen(url_home_page).read()\n",
    "    titles= parse_webpage_at_given_scroll(html)\n",
    "    read_and_write_web(author,'en')\n",
    "    read_and_write_web(author,'ur')\n",
    "    read_and_write_web(author,'hi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
